WEEK 1:  TOKENIZING TEXT

How to turn text into sequences of numbers:
a number was the value of a key value pair with the key being the word. 

For example: you could represent the word Hello with the value three, and then replace every instance of the word with a three in a sequence. 

For this week, I just learnet how to compute a string of numbers representing words in a sequence

try:
   ...
except:
   pass

WEEK 2: embedding

What are embeddings?

A process to help get sentiment out of text. This can be learned from a group of words in the same way as features are extracted from images to learn some patterns. This process is called embedding,


Words and similar words (with the same sentiment) are clustered as vectors in a multi-dimensional space. SO 1 cluster will have similar words (eg., boring, annoying, etc.)

For example: the word dog can be represented by a vector pointing in one direction; the word canine would also be represented by another vector pointing in a similar direction; so you could conclude that their sentiment is similar

Prepare data:
MAX_LENGTH = X: each sentence will be = X; If review is longer, it is truncated; if shorter, it is padded

Model:
- tf.keras.layers.Embedding(): output will be a 2D array with the length of the sentence and the embedding dimension, for example 16, as its size: (None, max_length, embedding_size)=(None, 120, 16)
After:
- flatten(): use 1d instead since it averages across the vector and so the optut size will be 16, must lower than the one obtained from the flatten layer
- To have a flatten() layer, you need to add an input_length to the embedding layer before;
- the last layer is a dense layer with 1 neuron (since it's a binary classification problem) and the activation is sigmoid (to give a value between 0 and 1);


Demonstrate the words and their embeddings (embeddings of each word in our index):
#First: results of the embeddings layer (layer 0)
e = model.layers[0]
w = e.get_weights()[0] #(10000, 16) array; each weight, (1,16), is an array of that specific word

#Function to get the word from the key number
reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])

Than you save:
- word gotten from the reverse_word_index function;
- embedding (1,16) array, gotten from the weight array of that word


